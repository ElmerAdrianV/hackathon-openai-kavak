````markdown
# Agentic Movie Recommender ‚Äî Debate + Judge (Boilerplate)

This is a **scaffold** for the Debate + Judge recommender with an **Orchestrator agent**,
**Critic tool-agents**, **Judge tool-agents**, a **Calibrator** head, and a **Router** (bandit policy).
It is intentionally minimal so you can plug in your own prompts, rules, and update logic.

## üéØ Key Features

- ‚úÖ **Train/Test Split Support**: Prevents data contamination during evaluation
- ‚úÖ **Multi-Critic Debate**: Multiple critic personas analyze movies from different perspectives
- ‚úÖ **Judge Aggregation**: Judges synthesize critic opinions with skill tracking
- ‚úÖ **Online Learning**: Calibrator and judges update from user feedback
- ‚úÖ **Relative Path Resolution**: Works consistently regardless of execution directory

## Layout
```
agentic_rec/
  ‚îú‚îÄ docs/
  ‚îÇ   ‚îú‚îÄ arquitectura.md
  ‚îÇ   ‚îî‚îÄ train_test_split.md  # üìñ Train/test split documentation
  ‚îú‚îÄ src/
  ‚îÇ   ‚îú‚îÄ data/
  ‚îÇ   ‚îÇ   ‚îî‚îÄ splits/
  ‚îÇ   ‚îÇ       ‚îú‚îÄ train_users_1.csv  # Training data
  ‚îÇ   ‚îÇ       ‚îî‚îÄ test_users_1.csv   # Test data (no contamination)
  ‚îÇ   ‚îú‚îÄ resources/
  ‚îÇ   ‚îÇ   ‚îú‚îÄ movie_critics/    # Critic persona prompts
  ‚îÇ   ‚îÇ   ‚îî‚îÄ judges/           # Judge prompts
  ‚îÇ   ‚îú‚îÄ orchestrator.py       # primary agent: routing, debate, aggregation, updates
  ‚îÇ   ‚îú‚îÄ critics.py            # critic personae + manager
  ‚îÇ   ‚îú‚îÄ judges.py             # judges + skill tracking
  ‚îÇ   ‚îú‚îÄ calibrator.py         # online regressor + uncertainty head
  ‚îÇ   ‚îú‚îÄ router.py             # routing/bandit policy for critics/judges
  ‚îÇ   ‚îú‚îÄ retriever.py          # user/movie context retrieval
  ‚îÇ   ‚îú‚îÄ data_store.py         # üîí Train/test aware data storage
  ‚îÇ   ‚îú‚îÄ features.py           # feature builders for calibrator
  ‚îÇ   ‚îú‚îÄ logging_store.py      # event logging and replay helpers
  ‚îÇ   ‚îú‚îÄ types.py              # dataclasses for typed IO
  ‚îÇ   ‚îú‚îÄ llm_client.py         # interface to your LLM provider
  ‚îÇ   ‚îú‚îÄ validate_split.py     # üîç Data contamination checker
  ‚îÇ   ‚îî‚îÄ main_demo.py          # runnable demo
  ‚îî‚îÄ requirements.txt
```

## Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Set Up OpenAI API Key
```bash
export OPENAI_API_KEY='sk-...'
# or create a .env file with OPENAI_API_KEY=sk-...
```

### 3. Run Demo (Automatic Train/Test Split)
```bash
python3 -m src.main_demo
```

The system will:
- Automatically detect `src/data/splits/train_users_1.csv` and `test_users_1.csv`
- Build user context ONLY from train data (no contamination)
- Evaluate predictions on test data
- Display predictions vs ground truth

### 4. Validate Data Integrity
```bash
python3 src/validate_split.py
```

This ensures no (userId, movieId) pairs overlap between train and test.

## üîí Train/Test Split Architecture

The system prevents data contamination by:

- **Train data** ‚Üí Used for user history, neighbors, popularity metrics
- **Test data** ‚Üí Only movie metadata accessible; ratings NEVER exposed during prediction
- **Movie metadata** ‚Üí Available from both train and test (title, synopsis, genres)

See [docs/train_test_split.md](docs/train_test_split.md) for detailed documentation.

## Usage Examples

### Default Mode (Train/Test Split)
```bash
# Uses src/data/splits/train_users_1.csv + test_users_1.csv
python3 -m src.main_demo
```

### Single File Mode (Backward Compatible)
```bash
# Use single file as train data
python3 -m src.main_demo data/my_ratings.csv
```

### Custom Resources Directory
```bash
# Specify custom critic/judge prompts location
python3 -m src.main_demo --resources path/to/resources
```

### With Verbose Output
```bash
# See detailed critic/judge outputs
VERBOSE=1 python3 -m src.main_demo
```

## Development

### Adding New Critics

1. Create a new prompt file in `src/resources/movie_critics/`:
```bash
echo "You are a Horror Movie Expert critic..." > src/resources/movie_critics/horror_expert.txt
```

2. The system auto-discovers and loads it on next run.

### Adding New Judges

1. Create a new prompt file in `src/resources/judges/`:
```bash
echo "You are a Consensus Judge..." > src/resources/judges/consensus_v1.txt
```

2. Auto-discovered on next run.

### Testing Changes

```bash
# Check for syntax errors
python3 -m py_compile src/*.py

# Validate data integrity
python3 src/validate_split.py

# Run demo with verbose output
VERBOSE=1 python3 -m src.main_demo
```

## Architecture Notes

- **Calibrator**: Tiny online linear regressor (no external ML deps), updates from user feedback
- **Router**: Bandit-style policy for selecting critics/judges based on genre and performance
- **Judge Skill Tracking**: Judges maintain skill estimates that improve with feedback
- **Critic Performance**: Track critic accuracy over time (exponential moving average)
- **Logging**: All predictions/debates logged to `./logs/events.jsonl`

## Data Format

### Required CSV Columns

```
userId,movieId,rating,title,overview,genres,genre_list,personality
1,858,5.0,Sleepless in Seattle,"A young boy...",<json>,"['Comedy','Drama','Romance']","Drama fan..."
```

- `userId`: User identifier (any type, converted to string)
- `movieId`: Movie identifier (any type, converted to string)
- `rating`: User rating (float, typically 0-5)
- `title`: Movie title (string)
- `overview`: Movie synopsis (string)
- `genre_list`: Parsed list of genre strings
- `personality`: User personality/profile description (optional)

## Notes

- Everything is typed for clarity but simplified for portability
- Relative paths work regardless of execution directory
- Train/test split is optional but highly recommended for evaluation
- System is backward compatible with single-file datasets

## Next Steps

1. ‚úÖ Set up train/test splits in `src/data/splits/`
2. ‚úÖ Add your LLM API key
3. ‚ö†Ô∏è Customize critic prompts in `src/resources/movie_critics/`
4. ‚ö†Ô∏è Customize judge prompts in `src/resources/judges/`
5. ‚ö†Ô∏è Implement advanced update logic in `orchestrator.py`
6. ‚ö†Ô∏è Add evaluation metrics (MAE, RMSE, coverage)

````
